# -*- coding: utf-8 -*-
"""Deepfake Base Project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zQ355dqdmo1BPg2Z3V39RC8OWMqtXzHv
"""

import torch
from torch import nn
import torchvision.transforms as transforms
from torchvision import transforms, models
import cv2
import os
import random
from sklearn.model_selection import train_test_split
from pathlib import Path
import torch.nn.functional as F
import PIL

from google.colab import drive
drive.mount('/content/drive')

# Define paths
input_fake_folder = Path("/content/drive/MyDrive/FF/fake")
input_real_folder = Path("/content/drive/MyDrive/FF/real")
output_folder = Path("/content/drive/MyDrive/FF/frames")
resnet50 = models.resnet50

"""##1. Preprocess Data (Extract Frames)
Firstly, we aim to preprocess our data. The `output_path` here should be the same for both real and fake because we are labelling the title. A concern may be that the data will all be real first then fake. However, we will shuffle it later.
"""

def getFrames(file_path, output_path, type_frame):
  vid = cv2.VideoCapture(str(file_path))
  curr_frame = 0
  skip = 3 # only every 5th frame

  # If frames output folder not created, then create it
  if not os.path.exists(output_path):
    os.makedirs(output_path)

  while True:
    # Reads next frame and
    success, frame = vid.read()

    # No next frame, then break
    if not success:
      break

    # Save as frame_real1.jpg
    if curr_frame % skip == 0:
      filename = f"{type_frame}_frame_{curr_frame:05d}.jpg"
      cv2.imwrite(os.path.join(output_path, filename), frame)
    curr_frame += 1

    # Exit
    if cv2.waitKey(1) & 0xFF == ord('q'):
      break

  # Release resources
  vid.release()
  cv2.destroyAllWindows()

def getFramesFromFolder(folder_path, output_path, type):
  # Get all files in folder and convert it to a frame
  for file in os.listdir(folder_path):
    getFrames(folder_path + file, output_path, type)

"""### Sizing (More Preprocessing)

Now that we've gotten the frames, these frames need to be resized and augmented (optional) in order for our base model to be able to learn from it.

Let's define a transformation pipeline first for our frames.

The normalization values are based on the ImageNet dataset and will help us converge faster.
"""

transform = transforms.Compose([
    transforms.RandomCrop(196),
    transforms.RandomRotation(15),
    transforms.RandomHorizontalFlip(),
    transforms.ColorJitter(
        brightness=0.1,
        contrast=0.1,
        saturation=0.1,
        hue=0.05),
    transforms.ToTensor(),
    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])

"""Now we will get our image paths and create a training and testing split."""

image_paths = []

for file in os.listdir(output_folder):
  file_path = os.path.join(output_folder, file)
  if os.path.isfile(file_path):
    image_paths.append(file_path)

random.shuffle(image_paths)

train, test = train_test_split(image_paths, test_size=0.3)

class dataset(torch.utils.data.Dataset):
  def __init__(self, image_paths, transform):
    self.image_paths = image_paths
    self.transform = transform

  def __len__(self):
    return len(self.image_paths)

  def __getitem__(self, i):
    image = self.transform(PIL.Image.open(self.image_paths[i]))

    if "real" in self.image_paths[i]:
        label = 1
    else:
        label = 0

    return image, label

"""## 2. Model
Here, we implement the `Resnet` model.
"""

class CustomCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.resnet = resnet50(pretrained=True)

        for param in self.resnet.parameters():
            param.requires_grad = True

        num_ftrs = self.resnet.fc.in_features
        self.resnet.fc == nn.Identity
        self.dropout = nn.Dropout(p=0.2)
        self.fc1 = nn.Linear(1000, 128)
        self.fc2 = nn.Linear(128, 2)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.resnet(x)
        x = self.dropout(x)
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

"""Transform our data and input it into dataloaders."""

# Datasets to feed into dataloaders
train_dataset = dataset(train, transform)
test_dataset = dataset(test, transform)

# Creates batches for faster iteration
train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)
test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=8, shuffle=False, drop_last=True)

"""## 3. Model
Now, we train/test the model to analyze its performance.
"""

# Get our model
model = CustomCNN()

# Get our loss function and optimizer
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.00005, weight_decay=0.005)

epochs = 3
count = 0

for epoch in range(epochs):
  # Turn on training
  model.train()

  for batch in train_dataloader:
    images, labels = batch

    # Reset gradient back to zero
    optimizer.zero_grad()

    # Compute estimate
    y_hat = model(images)

    # Compute training loss
    train_loss = loss_fn(y_hat, labels)
    print("At epoch ", epoch, "The training loss is: ", train_loss)

    # Backpropogation
    train_loss.backward()

    # Weights move towards optimum
    optimizer.step()

# Turn on evaluation
model.eval()
with torch.no_grad():
  # Compute test loss through cross-entropy loss
  for image, labels in test_dataloader:
    y_hat = model(image)
    test_loss = loss_fn(y_hat, labels)
    probabilities = F.softmax(y_hat, dim=1)
    print(probabilities, labels, "\nNow the actual labels")
    predictions = torch.argmax(probabilities, dim=1)
    print(predictions, labels)
    print("The test loss is: ", test_loss)