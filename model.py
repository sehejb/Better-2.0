# -*- coding: utf-8 -*-
"""scikit-learn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/141Rn2JWNSWxVnQWL45za65dduXFS6cos
"""

import torch
from torch import nn
import torchvision.transforms as transforms
from torchvision import transforms, models
import cv2
import os
import random
from sklearn.model_selection import train_test_split
from pathlib import Path
import torch.nn.functional as F
import PIL

"""Linear Regression"""

from google.colab import drive
drive.mount('/content/drive')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define paths
input_fake_folder = Path("/content/drive/MyDrive/FF/fake")
input_real_folder = Path("/content/drive/MyDrive/FF/real")
output_folder = Path("/content/drive/MyDrive/FF/frames")
resnet50 = models.resnet50

def getFrames(file_path, output_path, type_frame):
  vid = cv2.VideoCapture(str(file_path))
  curr_frame = 0
  skip = 3 # only every 5th frame

  # If frames output folder not created, then create it
  if not os.path.exists(output_path):
    os.makedirs(output_path)

  while True:
    # Reads next frame and
    success, frame = vid.read()

    # No next frame, then break
    if not success:
      break

    # Save as frame_real1.jpg
    if curr_frame % skip == 0:
      filename = f"{type_frame}_frame_{curr_frame:05d}.jpg"
      cv2.imwrite(os.path.join(output_path, filename), frame)
    curr_frame += 1

    # Exit
    if cv2.waitKey(1) & 0xFF == ord('q'):
      break

  # Release resources
  vid.release()
  cv2.destroyAllWindows()

def getFramesFromFolder(folder_path, output_path, type):
  # Get all files in folder and convert it to a frame
  for file in os.listdir(folder_path):
    getFrames(folder_path + file, output_path, type)

transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])

image_paths = []

for file in os.listdir(output_folder):
  file_path = os.path.join(output_folder, file)
  if os.path.isfile(file_path):
    image_paths.append(file_path)

random.shuffle(image_paths)

train, test = train_test_split(image_paths, test_size=0.3)

class dataset(torch.utils.data.Dataset):
  def __init__(self, image_paths, transform):
    self.image_paths = image_paths
    self.transform = transform

  def __len__(self):
    return len(self.image_paths)

  def __getitem__(self, i):
    image = self.transform(PIL.Image.open(self.image_paths[i]))

    if "real" in self.image_paths[i]:
        label = 1
    else:
        label = 0

    return image, label

class CustomCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.resnet = resnet50(pretrained=True)

        for param in self.resnet.parameters():
            param.requires_grad = False

        num_ftrs = self.resnet.fc.in_features
        self.dropout = nn.Dropout(p=0.2)
        self.fc = nn.Linear(1000, 2)

    def forward(self, x):
        x = self.resnet(x)
        x = x.view(x.size(0), -1)
        x = self.dropout(x)
        x = self.fc(x)
        return x

# Datasets to feed into dataloaders
train_dataset = dataset(train, transform)
test_dataset = dataset(test, transform)

# Creates batches for faster iteration
train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)
test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, drop_last=True)

# Get our model
model = CustomCNN()

# Get our loss function and optimizer
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)

epochs = 2

for epoch in range(epochs):
  # Turn on training
  model.train()

  for batch in train_dataloader:
    images, labels = batch

    # Reset gradient back to zero
    optimizer.zero_grad()

    # Compute estimate
    y_hat = model(images)

    # Compute training loss
    train_loss = loss_fn(y_hat, labels)
    print("At epoch ", epoch, "The training loss is: ", train_loss)

    # Backpropogation
    train_loss.backward()

    # Weights move towards optimum
    optimizer.step()

# Turn on evaluation
model.eval()
with torch.no_grad():
  # Compute test loss through cross-entropy loss
  for image, labels in test_dataloader:
    y_hat = model(image)
    test_loss = loss_fn(y_hat, labels)
    probabilities = F.softmax(y_hat, dim=1)
    print(probabilities, labels, "\nNow the actual labels")
    predictions = torch.argmax(probabilities, dim=1)
    print(predictions, labels)
    print("The test loss is: ", test_loss)